{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing Labelled Data For SCEPTR Benchmarking And Finetuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sources\n",
    "- VDJdb (as of the 28th of January, 2024)\n",
    "- Preprocessed 10xGenomics whitepaper data (from Montemurro et al. 2023)\n",
    "\n",
    "## Inclusion criteria\n",
    "- Paired chain data\n",
    "\n",
    "## Exclusion criteria\n",
    "- 10xGenomics whitepaper (later replace with Morten Nielsen's ITRAP-filtered)\n",
    "- Non-human"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All data will be cleaned with tidytcells and only standardizable and functional TCR data will be passed on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pandas import DataFrame\n",
    "from pathlib import Path\n",
    "import tidytcells as tt\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tcr_data_path = Path(\"../tcr_data/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vdjdb = pd.read_csv(\n",
    "    tcr_data_path/\"raw\"/\"vdjdb\"/\"vdjdb_20240128.tsv\",\n",
    "    sep=\"\\t\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vdjdb.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "references = vdjdb[\"Reference\"].dropna()\n",
    "references_for_10x = references[references.str.contains(\"10x\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "references_for_10x.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(df: DataFrame) -> DataFrame:\n",
    "    df = enforce_exclusion_criteria(df)\n",
    "\n",
    "    df = group_paired_chains(df)\n",
    "    df = drop_rows_with_missing_data(df)\n",
    "    df = standardize_nomenclature(df)\n",
    "\n",
    "    df = df.reset_index(drop=True)\n",
    "    return df\n",
    "\n",
    "def enforce_exclusion_criteria(df: DataFrame) -> DataFrame:\n",
    "    df = remove_non_human_data(df)\n",
    "    df = remove_10x_data(df)\n",
    "    df = remove_single_chain_data(df)\n",
    "    return df.copy()\n",
    "\n",
    "def remove_non_human_data(df: DataFrame) -> DataFrame:\n",
    "    return df[df[\"Species\"] == \"HomoSapiens\"].copy()\n",
    "\n",
    "def remove_10x_data(df: DataFrame) -> DataFrame:\n",
    "    return df[df[\"Reference\"] != \"https://www.10xgenomics.com/resources/application-notes/a-new-way-of-exploring-immunity-linking-highly-multiplexed-antigen-recognition-to-immune-repertoire-and-phenotype/#\"].copy()\n",
    "\n",
    "def remove_single_chain_data(df: DataFrame) -> DataFrame:\n",
    "    return df[df[\"complex.id\"] != 0].copy()\n",
    "\n",
    "def group_paired_chains(df: DataFrame) -> DataFrame:\n",
    "    reformatted_rows = []\n",
    "\n",
    "    sc_complex_ids = df[\"complex.id\"].unique()\n",
    "    for complex_id in tqdm(sc_complex_ids):\n",
    "        tcr_info = df[df[\"complex.id\"] == complex_id]\n",
    "\n",
    "        if tcr_info.shape[0] != 2:\n",
    "            print(tcr_info)\n",
    "            raise RuntimeError\n",
    "\n",
    "        tra_info = tcr_info[tcr_info[\"Gene\"] == \"TRA\"].iloc[0]\n",
    "        trb_info = tcr_info[tcr_info[\"Gene\"] == \"TRB\"].iloc[0]\n",
    "\n",
    "        reformatted_rows.append(\n",
    "            {\n",
    "                \"TRAV\": tra_info[\"V\"],\n",
    "                \"CDR3A\": tra_info[\"CDR3\"],\n",
    "                \"TRAJ\": tra_info[\"J\"],\n",
    "                \"TRBV\": trb_info[\"V\"],\n",
    "                \"CDR3B\": trb_info[\"CDR3\"],\n",
    "                \"TRBJ\": trb_info[\"J\"],\n",
    "                \"Epitope\": tra_info[\"Epitope\"],\n",
    "                \"MHCA\": tra_info[\"MHC A\"],\n",
    "                \"MHCB\": tra_info[\"MHC B\"],\n",
    "                \"Reference\": tra_info[\"Reference\"]\n",
    "            }\n",
    "        )\n",
    "\n",
    "    reformatted_df = DataFrame.from_records(reformatted_rows)\n",
    "    reformatted_df = reformatted_df.drop_duplicates()\n",
    "    return reformatted_df\n",
    "\n",
    "def drop_rows_with_missing_data(df: DataFrame) -> DataFrame:\n",
    "    return df.dropna(subset=[\"TRAV\", \"CDR3A\", \"TRAJ\", \"TRBV\", \"CDR3B\", \"TRBJ\", \"Epitope\"])\n",
    "\n",
    "def standardize_nomenclature(df: DataFrame) -> DataFrame:\n",
    "    df[\"TRAV\"] = df[\"TRAV\"].map(lambda x: tt.tr.standardize(x, enforce_functional=True))\n",
    "    df[\"TRAJ\"] = df[\"TRAJ\"].map(lambda x: tt.tr.standardize(x, enforce_functional=True))\n",
    "    df[\"TRBV\"] = df[\"TRBV\"].map(lambda x: tt.tr.standardize(x, enforce_functional=True))\n",
    "    df[\"TRBJ\"] = df[\"TRBJ\"].map(lambda x: tt.tr.standardize(x, enforce_functional=True))\n",
    "\n",
    "    df[\"CDR3A\"] = df[\"CDR3A\"].map(tt.junction.standardize)\n",
    "    df[\"CDR3B\"] = df[\"CDR3B\"].map(tt.junction.standardize)\n",
    "\n",
    "    df[\"MHCA\"] = df[\"MHCA\"].map(tt.mh.standardize)\n",
    "    df[\"MHCB\"] = df[\"MHCB\"].map(tt.mh.standardize)\n",
    "\n",
    "    return df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vdjdb_cleaned = preprocess(vdjdb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vdjdb_cleaned.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_10x = pd.read_csv(tcr_data_path/\"raw\"/\"10x_filtered\"/\"tcr.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_10x(df: DataFrame) -> DataFrame:\n",
    "    preprocessed = pd.DataFrame()\n",
    "\n",
    "    preprocessed[[\"TRAV\", \"TRAJ\"]] = df.apply(\n",
    "        lambda row: row[\"genes_TRA\"].split(\";\")[:2],\n",
    "        axis=\"columns\",\n",
    "        result_type=\"expand\"\n",
    "    ).map(lambda x: tt.tr.standardize(x, enforce_functional=True))\n",
    "    preprocessed[\"CDR3A\"] = df[\"cdr3_TRA\"].map(tt.junction.standardize)\n",
    "\n",
    "    preprocessed[[\"TRBV\", \"TRBJ\"]] = df.apply(\n",
    "        lambda row: (row[\"genes_TRB\"].split(\";\")[0], row[\"genes_TRB\"].split(\";\")[2]),\n",
    "        axis=\"columns\",\n",
    "        result_type=\"expand\"\n",
    "    ).map(lambda x: tt.tr.standardize(x, enforce_functional=True))\n",
    "    preprocessed[\"CDR3B\"] = df[\"cdr3_TRB\"].map(tt.junction.standardize)\n",
    "\n",
    "    preprocessed[[\"Epitope\", \"MHCA\"]] = df.apply(\n",
    "        lambda row: row[\"peptide_HLA\"].split(),\n",
    "        axis=\"columns\",\n",
    "        result_type=\"expand\"\n",
    "    )\n",
    "    preprocessed[\"MHCA\"] = preprocessed[\"MHCA\"].map(tt.mh.standardize)\n",
    "    preprocessed[\"MHCB\"] = \"B2M\"\n",
    "    preprocessed[\"Reference\"] = \"10x Genomics Whitepaper\"\n",
    "\n",
    "    return preprocessed.drop_duplicates().dropna(ignore_index=True)\n",
    "\n",
    "preprocessed_10x = preprocess_10x(clean_10x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessed_10x.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined = pd.concat([vdjdb_cleaned, preprocessed_10x], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vdjdb_cleaned.Epitope.nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vdjdb_cleaned.to_csv(tcr_data_path/\"preprocessed\"/\"benchmarking\"/\"vdjdb_cleaned.csv\", index=False)\n",
    "combined.to_csv(tcr_data_path/\"preprocessed\"/\"benchmarking\"/\"combined.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate specific split for SCEPTR finetuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "highly_sampled_epitopes = vdjdb_cleaned.groupby(\"Epitope\").filter(lambda ep_group: len(ep_group) > 300 and ep_group[\"Reference\"].nunique() > 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_valid_split = []\n",
    "\n",
    "for epitope, tcr_indices in highly_sampled_epitopes.groupby(\"Epitope\").groups.items():\n",
    "    tcrs = highly_sampled_epitopes.loc[tcr_indices]\n",
    "    num_tcrs_per_reference = tcrs.groupby(\"Reference\").size().sort_values(ascending=False)\n",
    "    cumulative_num_tcrs = num_tcrs_per_reference.cumsum()\n",
    "    \n",
    "    references_to_use_for_training = []\n",
    "\n",
    "    enough_training_data = False\n",
    "    for reference, cumsum in cumulative_num_tcrs.items():\n",
    "        if enough_training_data:\n",
    "            break\n",
    "\n",
    "        references_to_use_for_training.append(reference)\n",
    "        if cumsum > 200:\n",
    "            enough_training_data = True\n",
    "    \n",
    "    train_split_for_epitope = tcrs[tcrs[\"Reference\"].map(lambda x: x in references_to_use_for_training)]\n",
    "    train_valid_split.append(train_split_for_epitope)\n",
    "\n",
    "train_valid_split = pd.concat(train_valid_split)\n",
    "test_split = vdjdb_cleaned[~vdjdb_cleaned.index.isin(train_valid_split.index)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set(train_valid_split.index).intersection(set(test_split.index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(vdjdb_cleaned) == len(train_valid_split) + len(test_split)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_split = train_valid_split.groupby(\"Epitope\").sample(n=200, random_state=420)\n",
    "valid_split = train_valid_split[~train_valid_split.index.isin(train_split.index)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set(train_split.index).intersection(set(valid_split.index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(train_valid_split) == len(train_split) + len(valid_split)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_split[test_split.Epitope.map(lambda x: x in train_valid_split.Epitope.unique())].groupby(\"Epitope\").aggregate({\"Reference\": \"unique\"}).to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_valid_split.groupby(\"Epitope\").aggregate({\"Reference\": \"unique\"}).to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_split.groupby(\"Epitope\").size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_valid_split.to_csv(tcr_data_path/\"preprocessed\"/\"benchmarking\"/\"train_valid.csv\", index=False)\n",
    "train_split.to_csv(tcr_data_path/\"preprocessed\"/\"benchmarking\"/\"train.csv\", index=False)\n",
    "valid_split.to_csv(tcr_data_path/\"preprocessed\"/\"benchmarking\"/\"valid.csv\", index=False)\n",
    "test_split.to_csv(tcr_data_path/\"preprocessed\"/\"benchmarking\"/\"test.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
